## üß† Explicaci√≥n T√©cnica

La aplicaci√≥n usa una arquitectura **RAG (Retrieval-Augmented Generation)** para evitar enviar al modelo grandes porciones del repositorio de Cal.com. En lugar de intentar meter todo el contenido en el prompt, se indexa el c√≥digo de forma inteligente y solo se env√≠an los fragmentos realmente relevantes.

> üí° **Nota sobre el modelo:**  
> Actualmente se utiliza **`gpt-5.1-mini`** por motivos de **coste**. Esto implica que:
> - No se env√≠a **todo** el historial de conversaci√≥n en cada request, sino solo un historial limitado.
> - La **calidad de las respuestas** puede mejorar si se usa un modelo m√°s caro y potente (por ejemplo, un modelo ‚Äúfull‚Äù de la familia GPT-5.1) y/o si se env√≠a m√°s contexto en cada llamada.

---

### 1. Indexaci√≥n del repositorio (pre-proceso)

- Se clona el repo p√∫blico `cal.com`.  
- Con **Tree-sitter** se parsean archivos de varias extensiones (TS/JS/JSON, etc.) para extraer unidades l√≥gicas como:
  - funciones  
  - clases  
  - componentes  
  - imports/exports  
- Para cada fragmento se genera una **descripci√≥n compacta** mediante el modelo, usando la estructura detectada como insumo.  
- Esa descripci√≥n se convierte en un **embedding** y se almacena en un √≠ndice vectorial junto con su metadata, que incluye:
  - el **path** del archivo origen  
  - la **descripci√≥n generada** del fragmento  
- Este preprocesamiento se ejecuta una sola vez; en tiempo de consulta no se vuelven a generar embeddings del repositorio.

---

### 2. Flujo de consulta

1) El usuario env√≠a un mensaje.  
2) El modelo analiza la consulta y decide si necesita hacer una b√∫squeda externa.  
   - Si requiere contexto del repositorio, genera el embedding de la consulta y puede invocar el **descriptionFileTool** para obtener fragmentos relevantes mediante b√∫squeda sem√°ntica.
   - Si la pregunta exige leer un archivo, listar una carpeta o buscar coincidencias exactas, puede elegir **contentFileTool**, **readFolderTool** o **findInRepoTool**.
   - Si no necesita informaci√≥n del repositorio, responde directamente sin usar herramientas.
3) Con la informaci√≥n recuperada (si la hay), el modelo construye un **prompt reducido**, que incluye solo los fragmentos esenciales y un resumen breve del historial reciente (no se env√≠a la conversaci√≥n completa para optimizar costes).
4) El modelo responde en **streaming**, manteniendo un contexto liviano y evitando cargar contenido innecesario del repositorio.

Este dise√±o permite que el agente consulte el repo √∫nicamente cuando la tarea lo demanda, combinando RAG sem√°ntico con herramientas de exploraci√≥n directa del c√≥digo sin saturar el l√≠mite de tokens ni disparar los costes de uso del modelo.
